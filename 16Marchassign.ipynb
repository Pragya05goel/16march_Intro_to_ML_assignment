{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e58096b-e0f7-48f4-8035-3f8bad367d63",
   "metadata": {},
   "source": [
    "# **ASSIGNMENT**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774ba457-8193-47b5-945f-593a12aacb5c",
   "metadata": {},
   "source": [
    "**Q1: Define overfitting and underfitting in machine learning. What are the consequences of each, and how\n",
    "can they be mitigated?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77fc5147-09ec-4ed7-b6b3-3eed7909363a",
   "metadata": {},
   "source": [
    "Underfitting:\n",
    "\n",
    "A statistical model or a machine learning algorithm is said to have underfitting when it cannot capture the underlying trend of the data, i.e., it only performs well on training data but performs poorly on testing data. Underfitting destroys the accuracy of our machine learning model. Its occurrence simply means that our model or the algorithm does not fit the data well enough. It usually happens when we have fewer data to build an accurate model and also when we try to build a linear model with fewer non-linear data. In such cases, the rules of the machine learning model are too easy and flexible to be applied to such minimal data and therefore the model will probably make a lot of wrong predictions. Underfitting can be avoided by using more data and also reducing the features by feature selection. \n",
    "\n",
    "In a nutshell, Underfitting refers to a model that can neither performs well on the training data nor generalize to new data.\n",
    "\n",
    "Reasons for Underfitting:\n",
    "\n",
    ". High bias and low variance \n",
    "\n",
    ". The size of the training dataset used is not enough.\n",
    "\n",
    ". The model is too simple.\n",
    "\n",
    ". Training data is not cleaned and also contains noise in it.\n",
    "\n",
    "Techniques to reduce underfitting: \n",
    "\n",
    ". Increase model complexity\n",
    "\n",
    ". Increase the number of features, performing feature engineering\n",
    "\n",
    ". Remove noise from the data.\n",
    "\n",
    ". Increase the number of epochs or increase the duration of training to get better results.\n",
    "\n",
    "\n",
    "Overfitting:\n",
    "\n",
    "A statistical model is said to be overfitted when the model does not make accurate predictions on testing data. When a model gets trained with so much data, it starts learning from the noise and inaccurate data entries in our data set. And when testing with test data results in High variance. Then the model does not categorize the data correctly, because of too many details and noise. The causes of overfitting are the non-parametric and non-linear methods because these types of machine learning algorithms have more freedom in building the model based on the dataset and therefore they can really build unrealistic models. A solution to avoid overfitting is using a linear algorithm if we have linear data or using the parameters like the maximal depth if we are using decision trees. \n",
    "\n",
    "In a nutshell, Overfitting is a problem where the evaluation of machine learning algorithms on training data is different from unseen data.\n",
    "\n",
    "Reasons for Overfitting are as follows:\n",
    "\n",
    ". High variance and low bias\n",
    "\n",
    ". The model is too complex\n",
    "\n",
    ". The size of the training data \n",
    "\n",
    "Techniques to reduce overfitting:\n",
    "\n",
    ". Increase training data.\n",
    "\n",
    ". Reduce model complexity.\n",
    "\n",
    ". Early stopping during the training phase (have an eye over the loss over the training period as soon as loss begins to increase stop training).\n",
    "\n",
    ". Ridge Regularization and Lasso Regularization\n",
    "\n",
    ". Use dropout for neural networks to tackle overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55713ef-c0a1-4cf9-8215-b60324996bc3",
   "metadata": {},
   "source": [
    "**Q2: How can we reduce overfitting? Explain in brief.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56ba809-5fe5-4c42-ab6c-732fb47a2c37",
   "metadata": {},
   "source": [
    "Overfitting:\n",
    "\n",
    "A statistical model is said to be overfitted when the model does not make accurate predictions on testing data. When a model gets trained with so much data, it starts learning from the noise and inaccurate data entries in our data set. And when testing with test data results in High variance. Then the model does not categorize the data correctly, because of too many details and noise. The causes of overfitting are the non-parametric and non-linear methods because these types of machine learning algorithms have more freedom in building the model based on the dataset and therefore they can really build unrealistic models. A solution to avoid overfitting is using a linear algorithm if we have linear data or using the parameters like the maximal depth if we are using decision trees. \n",
    "\n",
    "In a nutshell, Overfitting is a problem where the evaluation of machine learning algorithms on training data is different from unseen data.\n",
    "\n",
    "Reasons for Overfitting are as follows:\n",
    "\n",
    ". High variance and low bias\n",
    "\n",
    ". The model is too complex\n",
    "\n",
    ". The size of the training data \n",
    "\n",
    "Techniques to reduce overfitting:\n",
    "\n",
    ". Increase training data.\n",
    "\n",
    ". Reduce model complexity.\n",
    "\n",
    ". Early stopping during the training phase (have an eye over the loss over the training period as soon as loss begins to increase stop training).\n",
    "\n",
    ". Ridge Regularization and Lasso Regularization\n",
    "\n",
    ". Use dropout for neural networks to tackle overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cedfb234-4412-4cf5-a2a0-29b42290563f",
   "metadata": {},
   "source": [
    "**Q3: Explain underfitting. List scenarios where underfitting can occur in ML.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba318ff8-97af-404b-8e2a-a2d45741864c",
   "metadata": {},
   "source": [
    "Underfitting:\n",
    "\n",
    "A statistical model or a machine learning algorithm is said to have underfitting when it cannot capture the underlying trend of the data, i.e., it only performs well on training data but performs poorly on testing data. Underfitting destroys the accuracy of our machine learning model. Its occurrence simply means that our model or the algorithm does not fit the data well enough. It usually happens when we have fewer data to build an accurate model and also when we try to build a linear model with fewer non-linear data. In such cases, the rules of the machine learning model are too easy and flexible to be applied to such minimal data and therefore the model will probably make a lot of wrong predictions. Underfitting can be avoided by using more data and also reducing the features by feature selection. \n",
    "\n",
    "In a nutshell, Underfitting refers to a model that can neither performs well on the training data nor generalize to new data.\n",
    "\n",
    "Reasons for Underfitting:\n",
    "\n",
    ". High bias and low variance \n",
    "\n",
    ". The size of the training dataset used is not enough.\n",
    "\n",
    ". The model is too simple.\n",
    "\n",
    ". Training data is not cleaned and also contains noise in it.\n",
    "\n",
    "Techniques to reduce underfitting: \n",
    "\n",
    ". Increase model complexity\n",
    "\n",
    ". Increase the number of features, performing feature engineering\n",
    "\n",
    ". Remove noise from the data.\n",
    "\n",
    ". Increase the number of epochs or increase the duration of training to get better results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be91c4cd-b086-44cd-85ab-f3801a23a257",
   "metadata": {},
   "source": [
    "**Q4: Explain the bias-variance tradeoff in machine learning. What is the relationship between bias and\n",
    "variance, and how do they affect model performance?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a3e5a2-6c9d-4e4c-b4df-6aaf0b02d3f8",
   "metadata": {},
   "source": [
    "Bias Variance Tradeoff\n",
    "\n",
    "If the algorithm is too simple (hypothesis with linear eq.) then it may be on high bias and low variance condition and thus is error-prone. If algorithms fit too complex ( hypothesis with high degree eq.) then it may be on high variance and low bias. In the latter condition, the new entries will not perform well. Well, there is something between both of these conditions, known as Trade-off or Bias Variance Trade-off.\n",
    "\n",
    "This tradeoff in complexity is why there is a tradeoff between bias and variance. An algorithm can’t be more complex and less complex at the same time.\n",
    "\n",
    "Bias:\n",
    "\n",
    "\n",
    "The bias is known as the difference between the prediction of the values by the ML model and the correct value. Being high in biasing gives a large error in training as well as testing data. Its recommended that an algorithm should always be low biased to avoid the problem of underfitting.\n",
    "By high bias, the data predicted is in a straight line format, thus not fitting accurately in the data in the data set. Such fitting is known as Underfitting of Data. This happens when the hypothesis is too simple or linear in nature.\n",
    "\n",
    "\n",
    "Variance:\n",
    "\n",
    "\n",
    "The variability of model prediction for a given data point which tells us spread of our data is called the variance of the model. The model with high variance has a very complex fit to the training data and thus is not able to fit accurately on the data which it hasn’t seen before. As a result, such models perform very well on training data but has high error rates on test data.\n",
    "When a model is high on variance, it is then said to as Overfitting of Data. Overfitting is fitting the training set accurately via complex curve and high order hypothesis but is not the solution as the error with unseen data is high.\n",
    "While training a data model variance should be kept low."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cffecd6-540b-4277-95dc-fe542ffd8c23",
   "metadata": {},
   "source": [
    "**Q5: Discuss some common methods for detecting overfitting and underfitting in machine learning models.\n",
    "How can you determine whether your model is overfitting or underfitting?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7bf86a-fd1b-4288-a7c0-a300cb28fa2e",
   "metadata": {},
   "source": [
    "We can determine whether a predictive model is underfitting or overfitting the training data by looking at the prediction error on the training data and the evaluation data.\n",
    "\n",
    "Our model is underfitting the training data when the model performs poorly on the training data. This is because the model is unable to capture the relationship between the input examples (often called X) and the target values (often called Y). our model is overfitting your training data when you see that the model performs well on the training data but does not perform well on the evaluation data. This is because the model is memorizing the data it has seen and is unable to generalize to unseen examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d72a50-2411-4810-9d7c-d0caae9cb8f3",
   "metadata": {},
   "source": [
    "**Q6: Compare and contrast bias and variance in machine learning. What are some examples of high bias\n",
    "and high variance models, and how do they differ in terms of their performance?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375a110c-4a4b-4bd8-abf2-cbe5b75d51d5",
   "metadata": {},
   "source": [
    "Bias and variance are two important concepts in machine learning that relate to the performance and generalization ability of a model. Here's a comparison and contrast between bias and variance:\n",
    "\n",
    "Bias:\n",
    "- Bias refers to the error introduced by approximating a real-world problem with a simplified model.\n",
    "- A model with high bias tends to make strong assumptions or oversimplifications about the data, leading to underfitting.\n",
    "- High bias models have limited capacity to capture complex patterns in the data.\n",
    "- These models may overlook important relationships and exhibit high systematic error or training error.\n",
    "- Examples of high bias models include linear regression with few features, simple decision trees with shallow depth, or logistic regression with limited flexibility.\n",
    "\n",
    "Variance:\n",
    "- Variance refers to the sensitivity of a model to fluctuations in the training data.\n",
    "- A model with high variance captures noise or random fluctuations in the training data, leading to overfitting.\n",
    "- High variance models have high capacity and can potentially fit the training data very well.\n",
    "- However, they may fail to generalize to unseen data and exhibit high variability or testing error.\n",
    "- Examples of high variance models include complex deep neural networks with many layers or decision trees with deep and highly branched structures.\n",
    "\n",
    "Differences in Performance:\n",
    "- High bias models tend to have low complexity and may have difficulty capturing intricate relationships in the data. As a result, they often underfit the training data and have high bias and high training error. However, these models may exhibit low variability and can generalize well to unseen data, resulting in lower testing error compared to high variance models.\n",
    "- High variance models, on the other hand, have high complexity and can capture complex patterns in the training data. They have low bias and can fit the training data very well, resulting in low training error. However, these models are more prone to overfitting and have high variability, leading to higher testing error compared to high bias models.\n",
    "\n",
    "The goal in machine learning is to strike a balance between bias and variance to achieve optimal model performance and generalization. This can be done through techniques like regularization, cross-validation, or ensemble methods (e.g., bagging and boosting) that aim to reduce overfitting and improve the trade-off between bias and variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb37ded7-74e9-48af-a678-d9d3e6b4824c",
   "metadata": {},
   "source": [
    "**Q7: What is regularization in machine learning, and how can it be used to prevent overfitting? Describe\n",
    "some common regularization techniques and how they work.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1612fc-038c-479d-9fc4-f3f64c7880c1",
   "metadata": {},
   "source": [
    "Regularization in machine learning is the process of regularizing the parameters that constrain, regularizes, or shrinks the coefficient estimates towards zero. In other words, this technique discourages learning a more complex or flexible model, avoiding the risk of Overfitting.\n",
    "\n",
    "\n",
    "It is a technique that improves model accuracy as well as prevents the loss of important data due to underfitting. When a model fails to grasp an underlying data trend, it is considered to be underfitting. The model does not fit enough points to produce accurate predictions. This means that it is likely to miss out on important data points that may have a telling impact on model accuracy. Hence we say important data may be lost as a result of underfitting.\n",
    "\n",
    "Regularization is a technique that adds information to a model to prevent the occurrence of overfitting. It is a type of regression that minimizes the coefficient estimates to zero to reduce the capacity (size) of a model. In this context, the reduction of the capacity of a model involves the removal of extra weights.\n",
    "\n",
    "Regularization removes extra weights from the selected features and redistributes the weights evenly. This means that regularization discourages the learning of a model of both high complexity and flexibility. A highly flexible model is one that possesses the freedom to fit as many data points as possible.\n",
    "\n",
    "Techniques of Regularization\n",
    "There are mainly two types of regularization techniques, which are given below:\n",
    "\n",
    "Ridge Regression\n",
    "Lasso Regression\n",
    "\n",
    "\n",
    "Ridge Regression:\n",
    "\n",
    "\n",
    ". Ridge regression is one of the types of linear regression in which a small amount of bias is introduced so that we can get better long-term predictions.\n",
    "\n",
    ".Ridge regression is a regularization technique, which is used to reduce the complexity of the model. It is also called as L2 regularization.\n",
    "\n",
    ".In this technique, the cost function is altered by adding the penalty term to it. The amount of bias added to the model is called Ridge Regression penalty. We can calculate it by multiplying with the lambda to the squared weight of each individual feature.\n",
    "\n",
    ".A general linear or polynomial regression will fail if there is high collinearity between the independent variables, so to solve such problems, Ridge regression can be used.\n",
    "\n",
    ".It helps to solve the problems if we have more parameters than samples.\n",
    "\n",
    "Lasso Regression:\n",
    "\n",
    "\n",
    ". Lasso regression is another regularization technique to reduce the complexity of the model. It stands for Least Absolute and Selection Operator.\n",
    "\n",
    ".It is similar to the Ridge Regression except that the penalty term contains only the absolute weights instead of a square of weights.\n",
    "\n",
    ".Since it takes absolute values, hence, it can shrink the slope to 0, whereas Ridge Regression can only shrink it near to 0.\n",
    "It is also called as L1 regularization.\n",
    "\n",
    ".Some of the features in this technique are completely neglected for model evaluation.\n",
    "\n",
    "Hence, the Lasso regression can help us to reduce the overfitting in the model as well as the feature selection.\n",
    "\n",
    "Differences between Ridge and Lasso:\n",
    "\n",
    "Ridge regression is mostly used to reduce the overfitting in the model, and it includes all the features present in the model. It reduces the complexity of the model by shrinking the coefficients.\n",
    "Lasso regression helps to reduce the overfitting in the model as well as feature selection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd24101-514b-4229-9bef-b721c1be656c",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2852796-681e-461d-81a3-a697dfb4efed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
